# Video Inference Pipeline

A production-ready video processing pipeline that demonstrates real-time object detection using containerized inference services, Apache Kafka for stream processing, and AWS infrastructure.

## ğŸ¯ Overview

This pipeline processes video streams through a distributed architecture, performing object detection on batched frames and storing annotated results in AWS S3.

## ğŸ—ï¸ Architecture

```
RTSP Server â†’ Frame Extractor â†’ Kafka â†’ Consumer â†’ Inference Service â†’ S3
     â†“              â†“                        â†“            â†“              â†“
 Video Stream   25-frame batches    Message Queue   Container API   Annotated Results
```

### Pipeline Flow

1. **RTSP Server** streams demo video via bluenviron/mediamtx
2. **Frame Extractor** batches 25 frames â†’ Kafka topic 'video-stream-1'
3. **Consumer** processes batches â†’ Containerized Inference Service
4. **Mock object detection** returns bounding boxes
5. **Post-processing** draws annotations (outside container)
6. **Results** uploaded to S3 as JSON with annotated images

## ğŸ”§ Technology Stack

- **Infrastructure**: AWS (EC2, S3, VPC) via Terraform
- **Messaging**: Apache Kafka (Confluent containers)
- **Inference**: Flask API + OpenCV (containerized with Docker)
- **Processing**: Python with kafka-python, boto3, PIL
- **Storage**: S3 with IAM roles and encryption

## ğŸ“‹ Infrastructure Components

| Component | Endpoint | Description |
|-----------|----------|-------------|
| RTSP Server | `<RTSP_IP>:8554` | Video streaming server |
| Kafka Cluster | `<KAFKA_IP>:9092` | Message broker (internal: 10.0.1.37:9092) |
| Inference Service | `<K8S_IP>:8080` | Containerized object detection API |
| S3 Bucket | `<S3_BUCKET>` | Result storage |

## ğŸ’¾ Repository Structure

```
.
â”œâ”€â”€ Core Pipeline
â”‚   â”œâ”€â”€ enhanced_extractor.py    # RTSP/file frame extraction (25-frame batching)
â”‚   â”œâ”€â”€ enhanced_consumer.py     # Kafka consumer with inference integration
â”‚   â”œâ”€â”€ inference_service.py     # Containerized CPU-based object detection
â”‚   â””â”€â”€ Dockerfile              # Container definition for inference service
â”‚
â”œâ”€â”€ Infrastructure as Code
â”‚   â”œâ”€â”€ terraform/              # Complete AWS infrastructure
â”‚   â”œâ”€â”€ k8s-deployment.yaml     # Kubernetes manifests
â”‚   â””â”€â”€ deploy_*.sh            # Deployment automation scripts
â”‚
â””â”€â”€ Testing & Verification
    â”œâ”€â”€ test_pipeline.sh        # Automated pipeline testing
    â””â”€â”€ final_summary.sh        # Deployment verification
```

## ğŸš€ Quick Start

### Prerequisites

- AWS CLI configured with appropriate credentials
- Terraform installed
- Docker installed
- Python 3.8+
- SSH key pair (`~/.ssh/company-key.pem`)

### Deploy Infrastructure

```bash
# Navigate to terraform directory
cd terraform

# Initialize and apply infrastructure
terraform init
terraform apply
```

### Run Pipeline Test

```bash
# Execute automated pipeline test
./test_pipeline.sh
```

### Manual Testing

```bash
# SSH into Kafka instance
ssh -i ~/.ssh/company-key.pem ec2-user@<KAFKA_IP>

# Start consumer in background
python3 consumer.py &

# Run extractor for 60 seconds
python3 extractor.py --source file --duration 60
```

## ğŸ³ Containerized Services

The pipeline uses Docker containers for:
- RTSP streaming server
- Kafka broker and Zookeeper
- Inference service API

Check container status:
```bash
# RTSP Server
ssh -i ~/.ssh/company-key.pem ec2-user@<RTSP_IP> "sudo docker ps"

# Kafka
ssh -i ~/.ssh/company-key.pem ec2-user@<KAFKA_IP> "sudo docker ps"

# Inference Service
ssh -i ~/.ssh/company-key.pem ec2-user@<K8S_IP> "sudo docker ps"
```

## ğŸ“Š Performance Monitoring

### Check S3 Results
```bash
aws s3 ls s3://<S3_BUCKET>/results/ --region us-east-1
```

### Inference Service Statistics
```bash
curl http://<K8S_IP>:8080/info
```

## âœ… Key Features

- âœ… RTSP video source streams to Kafka
- âœ… Kafka consumer batches 25 frames
- âœ… Containerized inference service (CPU-based)
- âœ… Consumer calls inference service with frame batches
- âœ… Post-processing draws bounding boxes (outside container)
- âœ… Annotated images uploaded to S3
- âœ… Infrastructure provisioned via Terraform (IaC)
- âœ… Clean GitHub repository with all components

## ğŸ” Verification

Run the final summary script to verify deployment:
```bash
./final_summary.sh
```

This will check:
- Infrastructure status
- Container health
- S3 results
- Service endpoints
- Performance metrics

## ğŸ“ License

[Add your license here]

## ğŸ¤ Contributing

[Add contribution guidelines here]

---

ğŸ’¡ **Note**: This pipeline demonstrates a complete, production-ready video processing workflow with containerized inference suitable for real-world applications.
